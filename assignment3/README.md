# ğŸµ Lyrics Generation with Deep Learning

## ×ª×™××•×¨ ×”×¤×¨×•×™×§×˜
×¤×¨×•×™×§×˜ ×–×” ××××© ××•×“×œ ×œ××™×“×” ×¢××•×§×” ×œ×™×¦×™×¨×ª ××™×œ×•×ª ×©×™×¨×™× ×‘×××¦×¢×•×ª ××¨×›×™×˜×§×˜×•×¨×ª RNN (Recurrent Neural Networks). ×”××•×“×œ ×××•××Ÿ ×¢×œ ×××’×¨ × ×ª×•× ×™× ×©×œ ××™×œ×•×ª ×©×™×¨×™× ×•×™×›×•×œ ×œ×™×¦×•×¨ ××™×œ×™× ×—×“×©×•×ª ×‘×”×ª×‘×¡×¡ ×¢×œ ×˜×§×¡×˜ ×”×ª×—×œ×ª×™ ×©× ×™×ª×Ÿ ×¢×œ ×™×“×™ ×”××©×ª××©.

**×¢×“×›×•× ×™× ×—×©×•×‘×™×:**
- âœ… **×ª×™×§×•×Ÿ Data Leakage**: ××™×œ×•×Ÿ ×”××™×œ×™× × ×‘× ×” ×¨×§ ×¢×œ × ×ª×•× ×™ ×”××™××•×Ÿ
- âœ… **TensorBoard Integration**: ××¢×§×‘ ××ª×§×“× ××—×¨ ×”××™××•×Ÿ ×‘××§×•× matplotlib
- âœ… **Professional Code Structure**: ×”×¢×¨×•×ª ××§×™×¤×•×ª ×•××¨×’×•×Ÿ ×‘×¨×•×¨

## ğŸ—ï¸ ××¨×›×™×˜×§×˜×•×¨×ª ×”×¤×¨×•×™×§×˜

```
assignment3/
â”œâ”€â”€ ğŸ“„ train.py                    # ×¡×§×¨×™×¤×˜ ×”××™××•×Ÿ ×”×¨××©×™ ×¢× TensorBoard
â”œâ”€â”€ ğŸ“„ generate.py                 # ×¡×§×¨×™×¤×˜ ×œ×’× ×¨×¦×™×” (×¢×ª×™×“ ×œ×”×ª×¤×ª×—)
â”œâ”€â”€ ğŸ“ data/
â”‚   â””â”€â”€ sets/
â”‚       â”œâ”€â”€ lyrics_train_set.csv   # × ×ª×•× ×™ ×”××™××•×Ÿ
â”‚       â””â”€â”€ lyrics_test_set.csv    # × ×ª×•× ×™ ×”×‘×“×™×§×”
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ RNN_baseline.py            # ××•×“×œ RNN ×”×‘×¡×™×¡×™
â”‚   â”œâ”€â”€ RNN_baseline_V1.py         # ×’×¨×¡×ª LSTM ×§×•× ×¡×¨×‘×˜×™×‘×™×ª
â”‚   â””â”€â”€ RNN_baseline_V2.py         # ×’×¨×¡×ª GRU ××’×¨×¡×™×‘×™×ª
â”œâ”€â”€ ğŸ“ utils/
â”‚   â”œâ”€â”€ text_utils.py              # ×›×œ×™× ×œ×¢×™×‘×•×“ ×˜×§×¡×˜ ××ª×§×“××™×
â”‚   â””â”€â”€ midi_features.py           # ×›×œ×™× ×œ×¢×™×‘×•×“ MIDI (×¢×ª×™×“)
â”œâ”€â”€ ğŸ“ embeddings/                 # ×ª×™×§×™×™×ª embeddings
â””â”€â”€ ğŸ“ runs/                       # TensorBoard logs
```

## ğŸ”¬ ×”×¤×™×™×¤×œ×™×™×Ÿ ×©×œ Data Science

### 1. **Data Loading & Preprocessing Pipeline** ğŸ“Š
**×§×•×‘×¥**: `utils/text_utils.py` + `train.py`

#### ×©×œ×‘×™×:
1. **`parse_lyrics_csv()`** - ×˜×•×¢×Ÿ ××™×œ×•×ª ×©×™×¨×™× ××§×‘×¦×™ CSV
   - ×§×•×¨× × ×ª×•× ×™× ××”×˜×‘×œ××•×ª
   - ×× ×§×” ×××•×¤×™×™× ×™× ×œ× ×¨×¦×•×™×™× ('&', ',,,,')
   - ××¡× ×Ÿ ××™×œ×•×ª ×©×™×¨×™× ×§×¦×¨×•×ª ××“×™

2. **`TextPreprocessor.clean_text()`** - × ×™×§×•×™ ×˜×§×¡×˜
   - ×”×•×¤×š ×˜×§×¡×˜ ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
   - ××¡×™×¨ ×¡×™×× ×™ ×¤×™×¡×•×§
   - ×× ×¨××œ ×¨×•×•×—×™×
   - ××‘×¦×¢ ×˜×•×§× ×™×–×¦×™×” ×‘×¡×™×¡×™×ª

3. **ğŸš¨ Data Leakage Prevention** - ×× ×™×¢×ª ×“×œ×™×¤×ª ××™×“×¢
   - **×‘×¢×‘×¨**: `all_lyrics = train_lyrics + test_lyrics` âŒ 
   - **×¢×›×©×™×•**: `preprocessor.build_vocabulary(train_lyrics)` âœ…
   - ×”××™×œ×•×Ÿ × ×‘× ×” ×¨×§ ×¢×œ × ×ª×•× ×™ ×”××™××•×Ÿ
   - × ×ª×•× ×™ ×”×˜×¡×˜ ××©×ª××©×™× ×‘××™×œ×•×Ÿ ×–×” (UNK ×œ××™×œ×™× ×œ× ××•×›×¨×•×ª)

4. **`TextPreprocessor.build_vocabulary()`** - ×‘× ×™×™×ª ××™×œ×•×Ÿ ××™×œ×™×
   - ×¡×¤×™×¨×ª ×ª×“×™×¨×•×ª ××™×œ×™× **×‘× ×ª×•× ×™ ×”××™××•×Ÿ ×‘×œ×‘×“**
   - ×™×¦×™×¨×ª ××™×¤×•×™ `word2idx` ×•-`idx2word`
   - ×”×•×¡×¤×ª ×˜×•×§× ×™× ××™×•×—×“×™×: `<PAD>`, `<UNK>`, `<SOS>`, `<EOS>`
   - ×¡×™× ×•×Ÿ ××™×œ×™× ×œ×¤×™ ×ª×“×™×¨×•×ª ××™× ×™××œ×™×ª

### 2. **Word Embeddings Pipeline** ğŸ”¤
**×§×•×‘×¥**: `utils/text_utils.py`

#### ×©×œ×‘×™×:
1. **`load_word2vec_embeddings()`** - ×˜×•×¢×Ÿ Word2Vec ××•×›×Ÿ
   - ××©×ª××© ×‘-`gensim.downloader` 
   - ××•×“×œ: `word2vec-google-news-300`
   - 300 ×××“×™× ×›× ×“×¨×©

2. **`_create_embedding_matrix()`** - ×™×¦×™×¨×ª ××˜×¨×™×¦×ª embeddings
   - ××˜×¨×™×¦×” ×‘×’×•×“×œ `[vocab_size, 300]`
   - ××™×œ×™× ×§×™×™××•×ª ×‘-Word2Vec: vector ××•×›×Ÿ
   - ××™×œ×™× ×œ× ×§×™×™××•×ª: vector ×¨× ×“×•××œ×™
   - PAD token: vector ××¤×¡×™×

### 3. **Training Data Preparation** ğŸ¯
**×§×•×‘×¥**: `utils/text_utils.py`

#### ×©×œ×‘×™×:
1. **`prepare_sequences()`** - ×™×¦×™×¨×ª ×¨×¦×¤×™× ×œ××™××•×Ÿ
   - ×™×¦×™×¨×ª sliding windows
   - ×›×œ ×¨×¦×£ ×”×•×¤×š ×œ××¡×¤×¨ ×“×•×’×××•×ª ××™××•×Ÿ
   - ×¤×•×¨××˜: `[context] â†’ next_word`
   - Padding ×œ××•×¨×š ××—×™×“

2. **Data Splitting** - ×—×œ×•×§×” × ×›×•× ×”:
   - Training: 80%
   - Validation: 10% 
   - Test: 10%
   - **×—×©×•×‘**: Test × ×©××¨ × ×¤×¨×“ ×œ×—×œ×•×˜×™×Ÿ

### 4. **Model Architecture Pipeline** ğŸ§ 
**×§×‘×¦×™×**: `models/RNN_baseline*.py`

#### ×’×¨×¡××•×ª ×”××•×“×œ:
```python
# RNN_baseline.py - ××•×“×œ ×‘×¡×™×¡×™ ×’××™×©
class LyricsRNN:
    - Embedding layer (300D Word2Vec)
    - LSTM/GRU layers (configurable)
    - Dropout layer 
    - Output projection (vocab_size)

# RNN_baseline_V1.py - LSTM ×§×•× ×¡×¨×‘×˜×™×‘×™
- LSTM, 2 layers, hidden=256, dropout=0.2
- Learning rate: 0.0005 (× ××•×š ×œ×™×¦×™×‘×•×ª)

# RNN_baseline_V2.py - GRU ××’×¨×¡×™×‘×™  
- GRU, 3 layers, hidden=512, dropout=0.4
- Learning rate: 0.001 (×’×‘×•×” ×œ××”×™×¨×•×ª)
```

### 5. **Training Pipeline with TensorBoard** ğŸ“ˆ
**×§×•×‘×¥**: `train.py`

#### ×©×™×¤×•×¨×™× ×—×“×©×™×:
1. **TensorBoard Logging** ×‘××§×•× matplotlib:
   ```python
   writer = SummaryWriter(log_dir=f'{log_dir}/lyrics_rnn_{timestamp}')
   
   # Batch-level logging
   writer.add_scalar('Loss/Train_Batch', loss, global_step)
   writer.add_scalar('Perplexity/Train_Batch', np.exp(loss), global_step)
   
   # Epoch-level logging  
   writer.add_scalars('Loss/Epoch', {
       'Training': avg_train_loss,
       'Validation': avg_val_loss
   }, epoch)
   ```

2. **Comprehensive Monitoring**:
   - Loss curves (train/validation)
   - Perplexity trends
   - Learning rate scheduling
   - Real-time progress tracking

3. **Early Stopping & Checkpointing**:
   - Monitor validation loss
   - Save best model state
   - Restore for evaluation

### 6. **Model Evaluation & Generation** ğŸ­
**×§×•×‘×¥**: `train.py`

#### ×©×œ×‘×™×:
1. **Test Set Evaluation**:
   - ×—×™×©×•×‘ Perplexity ×¢×œ × ×ª×•× ×™ ×˜×¡×˜ × ×§×™×™×
   - ××™×Ÿ data leakage

2. **Text Generation**:
   - Temperature sampling
   - Top-k sampling
   - ×”×“×’××” ×¢× seeds ×©×•× ×™×

## ğŸš€ ×”×¨×¦×ª ×”×¤×¨×•×™×§×˜

### ×”×ª×§× ×ª Dependencies
```bash
pip install torch torchvision torchaudio
pip install gensim pandas numpy tqdm tensorboard
```

### ×”×¨×¦×ª ××™××•×Ÿ
```bash
python train.py
```

### ×¦×¤×™×™×” ×‘-TensorBoard
```bash
tensorboard --logdir=runs
# ×¤×ª×— http://localhost:6006 ×‘×“×¤×“×¤×Ÿ
```

## ğŸ“Š ××¢×§×‘ ××—×¨ ×”×ª×§×“××•×ª

### TensorBoard Metrics:
1. **Loss/Train_Batch** - Loss ×œ×›×œ batch ×‘××™××•×Ÿ
2. **Loss/Epoch** - Loss ×××•×¦×¢ ×œ×›×œ epoch (train + validation)
3. **Perplexity/Epoch** - Perplexity ×œ×›×œ epoch
4. **Learning_Rate** - ×©×™× ×•×™×™× ×‘×§×¦×‘ ×œ××™×“×”

### ×¤×œ×˜×™× ×©×œ ×”××™××•×Ÿ:
```
models/
â”œâ”€â”€ best_lyrics_model.pth      # ×”××•×“×œ ×”×˜×•×‘ ×‘×™×•×ª×¨
â”œâ”€â”€ preprocessor.pkl           # ×¢×™×‘×•×“ ×”×˜×§×¡×˜
runs/
â””â”€â”€ lyrics_rnn_YYYYMMDD_HHMMSS/  # TensorBoard logs
```

## ğŸ¯ ×ª×•×¦××•×ª ×•××“×“×™ ×”×¢×¨×›×”

### ××“×“ ×¢×™×§×¨×™: Perplexity
- ×›×›×œ ×©×”×¢×¨×š × ××•×š ×™×•×ª×¨, ×”××•×“×œ ×˜×•×‘ ×™×•×ª×¨
- Perplexity = exp(loss)
- ×¢×¨×š ×˜×™×¤×•×¡×™ ×˜×•×‘: < 50

### ×”×©×•×•××ª ×’×¨×¡××•×ª:
- **V1 (LSTM)**: ×™×¦×™×‘×•×ª, ××™×›×•×ª ×˜×§×¡×˜ ×’×‘×•×”×”
- **V2 (GRU)**: ××”×™×¨×•×ª, ×™×¢×™×œ×•×ª ×–×™×›×¨×•×Ÿ

## ğŸ”§ ×”×ª×××•×ª ××™×©×™×•×ª

### ×©×™× ×•×™ ×”×’×“×¨×•×ª ×‘××•×“×œ:
```python
config = {
    'rnn_type': 'LSTM',      # ××• 'GRU'
    'hidden_size': 512,      # ×’×•×“×œ hidden state
    'num_layers': 2,         # ××¡×¤×¨ ×©×›×‘×•×ª  
    'dropout': 0.3,          # dropout rate
    'learning_rate': 0.001,  # ×§×¦×‘ ×œ××™×“×”
    'batch_size': 32,        # ×’×•×“×œ batch
}
```

## ğŸ“ ×”×¢×¨×•×ª ×˜×›× ×™×•×ª ×—×©×•×‘×•×ª

### Data Leakage Prevention:
- ××™×œ×•×Ÿ ×”××™×œ×™× × ×‘× ×” **×¨×§** ×¢×œ × ×ª×•× ×™ ×”××™××•×Ÿ
- × ×ª×•× ×™ ×”×˜×¡×˜ ××¢×•×‘×“×™× ×¢× ××™×œ×•×Ÿ ×–×” (UNK ×œ××™×œ×™× ×—×“×©×•×ª)
- ×–×” ××‘×˜×™×— ×©×”××•×“×œ ×œ× "×¨××”" ××ª × ×ª×•× ×™ ×”×˜×¡×˜ ××¨××©

### TensorBoard vs Matplotlib:
- TensorBoard: ××¢×§×‘ real-time, ××™× ×˜×¨××§×˜×™×‘×™, professional
- Matplotlib: ×¡×˜×˜×™, ×¤×©×•×˜ ×™×•×ª×¨, ×¤×—×•×ª ××™×“×¢
- TensorBoard ××ª××™× ×™×•×ª×¨ ×œ×¤×¨×•×™×§×˜×™ deep learning ××ª×§×“××™×

### Memory Management:
- ×”×©×™××•×© ×‘-DataLoaders ×××¤×©×¨ ×˜×¢×™× ×” ×—×›××” ×©×œ × ×ª×•× ×™×
- Gradient accumulation ××¤×©×¨×™ ×œbatches ×’×“×•×œ×™×
- GPU memory optimization ×¢× mixed precision

## ğŸµ ×“×•×’×××•×ª ×’× ×¨×¦×™×”

```python
# ×“×•×’×××•×ª ×œgenerates ××¦×•×¤×™×:
seeds = [
    "love is" â†’ "love is all we need to feel alive..."
    "music makes" â†’ "music makes the world go round and round..."
    "when the sun" â†’ "when the sun goes down the night begins..."
]
```

## ğŸ“š ×—×•××¨ ×¢×–×¨

- [TensorBoard Documentation](https://pytorch.org/docs/stable/tensorboard.html)
- [Data Leakage Prevention](https://machinelearningmastery.com/data-leakage-machine-learning/)
- [RNN for Text Generation](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html)

---
**××¤×ª×—**: ××˜×œ×” 3 - ×œ××™×“×” ×¢××•×§×” | **×¢×“×›×•×Ÿ**: ×™× ×•××¨ 2026
        
        # 4. Output Projection
        self.fc_out = nn.Linear(hidden_size, vocab_size)
```

#### ×–×¨×™××ª ×”××™×“×¢:
1. **Input**: ×¨×¦×£ ××™× ×“×§×¡×™× `[batch_size, seq_len]`
2. **Embedding**: `[batch_size, seq_len, 300]`
3. **RNN**: `[batch_size, seq_len, hidden_size]`
4. **Output**: `[batch_size, seq_len, vocab_size]`

### 5. **Training Pipeline** ğŸ‹ï¸â€â™‚ï¸
**×§×•×‘×¥**: `train.py` + `models/RNN_baseline.py`

#### LyricsRNNTrainer - ××—×œ×§×ª ×”××™××•×Ÿ:

**×©×œ×‘×™ ×”××™××•×Ÿ**:
1. **`train_step()`** - ×¦×¢×“ ××™××•×Ÿ ×™×—×™×“
   - Forward pass
   - ×—×™×©×•×‘ Loss (CrossEntropyLoss)
   - Backward propagation
   - Gradient clipping (max_norm=1.0)
   - Update weights

2. **`validate_step()`** - ×¦×¢×“ validation
   - Forward pass ×œ×œ× gradients
   - ×—×™×©×•×‘ validation loss

#### Training Loop ×‘×¤×•× ×§×¦×™×” `train_model()`:
```python
for epoch in range(num_epochs):
    # Training Phase
    for batch in train_loader:
        loss = trainer.train_step(input_batch, target_batch)
        
    # Validation Phase  
    for batch in val_loader:
        val_loss = trainer.validate_step(input_batch, target_batch)
        
    # Learning Rate Scheduling
    scheduler.step(avg_val_loss)
    
    # Early Stopping Check
    if val_loss < best_val_loss:
        save_best_model()
    else:
        patience_counter += 1
```

### 6. **Text Generation Pipeline** âœ¨
**×§×•×‘×¥**: `models/RNN_baseline.py`

#### ×¤×•× ×§×¦×™×™×ª `generate_text()`:
1. **Initialization**: ×˜×•×¢×Ÿ seed sequence
2. **Autoregressive Generation**:
   ```python
   for _ in range(max_length):
       # Forward pass
       output_logits = model(current_sequence)
       
       # Temperature scaling
       logits = logits / temperature
       
       # Top-k sampling
       top_k_logits, indices = torch.topk(logits, k)
       
       # Sample next word
       next_word = torch.multinomial(probabilities, 1)
       
       # Append to sequence
       generated_sequence.append(next_word)
   ```

### 7. **Evaluation Pipeline** ğŸ“Š
**×§×•×‘×¥**: `train.py`

#### ××“×“×™ ×”×¢×¨×›×”:
1. **Loss**: CrossEntropyLoss ×¢×œ test set
2. **Perplexity**: `exp(loss)` - ××“×“ ×œ××™ ×•×•×“××•×ª ×”××•×“×œ
3. **Generated Text Quality**: ×‘×“×™×§×” ××™×›×•×ª×™×ª ×©×œ ×˜×§×¡×˜ ×©× ×•×¦×¨

## ğŸ“ˆ ××˜×¨×™×§×•×ª ×•××“×“×™×

### Loss Function
```python
criterion = nn.CrossEntropyLoss(ignore_index=0)  # ××ª×¢×œ× ×-PAD tokens
```

### Perplexity Calculation
```python
perplexity = np.exp(cross_entropy_loss)
```
- ×¤×¨×¤×œ×§×¡×™×˜×™ × ××•×›×” = ××•×“×œ ×˜×•×‘ ×™×•×ª×¨
- ×¤×¨×¤×œ×§×¡×™×˜×™ ×©×œ ~100-200 × ×—×©×‘×ª ×˜×•×‘×” ×œ×’× ×¨×¦×™×™×ª ×˜×§×¡×˜

### Learning Rate Scheduling
```python
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)
```

## ğŸ›ï¸ ×”×™×¤×¨-×¤×¨××˜×¨×™×

```python
config = {
    'max_sequence_length': 50,    # ××•×¨×š ×¨×¦×£ ××§×¡×™××œ×™
    'batch_size': 32,             # ×’×•×“×œ batch
    'embedding_dim': 300,         # ×××“ Word2Vec
    'hidden_size': 512,           # ×’×•×“×œ hidden state
    'num_layers': 2,              # ××¡×¤×¨ ×©×›×‘×•×ª RNN
    'dropout': 0.3,               # ×§×¦×‘ dropout
    'learning_rate': 0.001,       # ×§×¦×‘ ×œ××™×“×”
    'min_word_freq': 2,           # ×ª×“×™×¨×•×ª ××™×œ×” ××™× ×™××œ×™×ª
}
```

## ğŸš€ ×”×¨×¦×ª ×”×¤×¨×•×™×§×˜

### ×“×¨×™×©×•×ª ××§×“×™××•×ª
```bash
pip install torch torchvision pandas numpy matplotlib seaborn gensim tqdm
```

### ×”×¨×¦×ª ××™××•×Ÿ
```bash
python train.py
```

### ×ª×”×œ×™×š ×”××™××•×Ÿ ×™×›×œ×•×œ:
1. âœ… ×˜×¢×™× ×ª × ×ª×•× ×™× ×•×¢×™×‘×•×“ ×˜×§×¡×˜
2. âœ… ×‘× ×™×™×ª ××™×œ×•×Ÿ ××™×œ×™×
3. âœ… ×˜×¢×™× ×ª Word2Vec embeddings  
4. âœ… ××™××•×Ÿ ×”××•×“×œ ×¢× early stopping
5. âœ… ×”×¢×¨×›×” ×¢×œ test set
6. âœ… ×’× ×¨×¦×™×™×ª ×“×•×’×××•×ª ×˜×§×¡×˜
7. âœ… ×©××™×¨×ª ××•×“×œ ×•××˜×¨×™×§×•×ª

## ğŸ“ ×¤×œ×˜×™× ×•×§×‘×¦×™×
- `models/best_lyrics_model.pth` - ×”××•×“×œ ×”×××•××Ÿ ×”×˜×•×‘ ×‘×™×•×ª×¨
- `models/preprocessor.pkl` - ×”preprocessor ×”×©××•×¨
- `training_curves.png` - ×’×¨×¤×™× ×©×œ loss ×•-perplexity
- Console output ×¢× ××“×“×™× ×•×“×•×’×××•×ª ×˜×§×¡×˜

## ğŸµ ×“×•×’×××•×ª ×’× ×¨×¦×™×”

×”××•×“×œ ×™×›×•×œ ×œ×™×¦×•×¨ ××™×œ×•×ª ×©×™×¨ ×‘×”×ª×‘×¡×¡ ×¢×œ ×˜×§×¡×˜ ×”×ª×—×œ×ª×™:

**Input**: "love is"  
**Output**: "love is a beautiful thing that makes me feel alive..."

**Input**: "in the night"  
**Output**: "in the night when stars are shining bright..."

## ğŸ”§ ×”×¨×—×‘×•×ª ×¢×ª×™×“×™×•×ª
- [ ] ××•×“×œ Transformer ×œ××™×œ×•×ª ×©×™×¨×™×
- [ ] ××™× ×˜×’×¨×¦×™×” ×¢× MIDI features
- [ ] ×××©×§ ××™× ×˜×¨××§×˜×™×‘×™ ×œ×’× ×¨×¦×™×”
- [ ] ××“×“×™ ×”×¢×¨×›×” ××™×›×•×ª×™×™× × ×•×¡×¤×™×

## ğŸ“š ××§×•×¨×•×ª ×•×”×©×¨××”
- ××¨×›×™×˜×§×˜×•×¨×ª RNN ××”×§×•×¨×¡ Deep Learning  
- Word2Vec embeddings ×-Google News
- ×˜×›× ×™×§×•×ª text generation ×¢×“×›× ×™×•×ª

---
**×¤×¨×•×™×§×˜ ×‘××¡×’×¨×ª**: ×”× ×“×¡×ª × ×ª×•× ×™× - ×œ××™×“×” ×¢××•×§×”, ×¡××¡×˜×¨ ×–'