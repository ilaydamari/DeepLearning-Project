# ğŸµ Lyrics Generation with Deep Learning 

## ×ª×™××•×¨ ×”×¤×¨×•×™×§×˜
×¤×¨×•×™×§×˜ ×–×” ××××© ××•×“×œ ×œ××™×“×” ×¢××•×§×” ×œ×™×¦×™×¨×ª ××™×œ×•×ª ×©×™×¨×™× ×‘×××¦×¢×•×ª ××¨×›×™×˜×§×˜×•×¨×ª RNN (Recurrent Neural Networks). ×”××•×“×œ ×××•××Ÿ ×¢×œ ×××’×¨ × ×ª×•× ×™× ×©×œ ××™×œ×•×ª ×©×™×¨×™× ×•×™×›×•×œ ×œ×™×¦×•×¨ ××™×œ×™× ×—×“×©×•×ª ×‘×”×ª×‘×¡×¡ ×¢×œ ×˜×§×¡×˜ ×”×ª×—×œ×ª×™ ×©× ×™×ª×Ÿ ×¢×œ ×™×“×™ ×”××©×ª××©.

## ğŸ—ï¸ ××¨×›×™×˜×§×˜×•×¨×ª ×”×¤×¨×•×™×§×˜

```
assignment3/
â”œâ”€â”€ ğŸ“„ train.py                    # ×¡×§×¨×™×¤×˜ ×”××™××•×Ÿ ×”×¨××©×™
â”œâ”€â”€ ğŸ“„ generate.py                 # ×¡×§×¨×™×¤×˜ ×œ×’× ×¨×¦×™×” (×¢×ª×™×“ ×œ×”×ª×¤×ª×—)
â”œâ”€â”€ ğŸ“ data/
â”‚   â””â”€â”€ sets/
â”‚       â”œâ”€â”€ lyrics_train_set.csv   # × ×ª×•× ×™ ×”××™××•×Ÿ
â”‚       â””â”€â”€ lyrics_test_set.csv    # × ×ª×•× ×™ ×”×‘×“×™×§×”
â”œâ”€â”€ ğŸ“ models/
â”‚   â”œâ”€â”€ RNN_baseline.py            # ××•×“×œ RNN ×”×‘×¡×™×¡×™
â”‚   â”œâ”€â”€ RNN_baseline_V1.py         # ×’×¨×¡××•×ª × ×•×¡×¤×•×ª ×©×œ ×”××•×“×œ
â”‚   â””â”€â”€ RNN_baseline_V2.py
â”œâ”€â”€ ğŸ“ utils/
â”‚   â”œâ”€â”€ text_utils.py              # ×›×œ×™× ×œ×¢×™×‘×•×“ ×˜×§×¡×˜
â”‚   â””â”€â”€ midi_features.py           # ×›×œ×™× ×œ×¢×™×‘×•×“ MIDI (×¢×ª×™×“)
â””â”€â”€ ğŸ“ embeddings/                 # ×ª×™×§×™×™×ª embeddings
```

## ğŸ”¬ ×”×¤×™×™×¤×œ×™×™×Ÿ ×©×œ Data Science

### 1. **Data Loading & Preprocessing Pipeline** ğŸ“Š
**×§×•×‘×¥**: `utils/text_utils.py` + `train.py`

#### ×©×œ×‘×™×:
1. **`parse_lyrics_csv()`** - ×˜×•×¢×Ÿ ××™×œ×•×ª ×©×™×¨×™× ××§×‘×¦×™ CSV
   - ×§×•×¨× × ×ª×•× ×™× ××”×˜×‘×œ××•×ª
   - ×× ×§×” ×××•×¤×™×™× ×™× ×œ× ×¨×¦×•×™×™× ('&', ',,,,')
   - ××¡× ×Ÿ ××™×œ×•×ª ×©×™×¨×™× ×§×¦×¨×•×ª ××“×™

2. **`TextPreprocessor.clean_text()`** - × ×™×§×•×™ ×˜×§×¡×˜
   - ×”×•×¤×š ×˜×§×¡×˜ ×œ××•×ª×™×•×ª ×§×˜× ×•×ª
   - ××¡×™×¨ ×¡×™×× ×™ ×¤×™×¡×•×§
   - ×× ×¨××œ ×¨×•×•×—×™×
   - ××‘×¦×¢ ×˜×•×§× ×™×–×¦×™×” ×‘×¡×™×¡×™×ª

3. **`TextPreprocessor.build_vocabulary()`** - ×‘× ×™×™×ª ××™×œ×•×Ÿ ××™×œ×™×
   - ×¡×¤×™×¨×ª ×ª×“×™×¨×•×ª ××™×œ×™×
   - ×™×¦×™×¨×ª ××™×¤×•×™ `word2idx` ×•-`idx2word`
   - ×”×•×¡×¤×ª ×˜×•×§× ×™× ××™×•×—×“×™×: `<PAD>`, `<UNK>`, `<SOS>`, `<EOS>`
   - ×¡×™× ×•×Ÿ ××™×œ×™× ×œ×¤×™ ×ª×“×™×¨×•×ª ××™× ×™××œ×™×ª

### 2. **Word Embeddings Pipeline** ğŸ”¤
**×§×•×‘×¥**: `utils/text_utils.py`

#### ×©×œ×‘×™×:
1. **`load_word2vec_embeddings()`** - ×˜×•×¢×Ÿ ××•×“×œ Word2Vec ××•×›×Ÿ
   - ××©×ª××© ×‘××•×“×œ Google News 300D
   - ×™×•×¦×¨ ××˜×¨×™×¦×ª embeddings
   
2. **`_create_embedding_matrix()`** - ×‘× ×™×™×ª ××˜×¨×™×¦×ª embeddings
   - ××˜×¨×™×¦×” ×‘×’×•×“×œ `[vocab_size, 300]`
   - ××ª×—×•×œ ×¢× Word2Vec ×¢×‘×•×¨ ××™×œ×™× ×§×™×™××•×ª
   - ××ª×—×•×œ ××§×¨××™ ×¢×‘×•×¨ ××™×œ×™× ×—×“×©×•×ª
   - ×•×§×˜×•×¨ ××¤×¡×™× ×¢×‘×•×¨ PAD token

### 3. **Sequence Preparation Pipeline** ğŸ“
**×§×•×‘×¥**: `utils/text_utils.py`

#### ×©×œ×‘×™×:
1. **`text_to_sequence()`** - ×”××¨×ª ×˜×§×¡×˜ ×œ×¨×¦×£ ××¡×¤×¨×™×
   - ×”×•×¡×¤×ª SOS token ×‘×”×ª×—×œ×”
   - ×”××¨×ª ××™×œ×™× ×œ××™× ×“×§×¡×™×
   - ×”×—×œ×¤×ª ××™×œ×™× ×œ× ××•×›×¨×•×ª ×‘-UNK token
   - ×”×•×¡×¤×ª EOS token ×‘×¡×•×£

2. **`prepare_sequences()`** - ×”×›× ×ª ×¨×¦×¤×™× ×œ××™××•×Ÿ
   - ×™×¦×™×¨×ª sliding window sequences
   - ×›×œ ×¨×¦×£ ×”×•× Input X ×•-Target Y (×”××™×œ×” ×”×‘××”)
   - Padding ×œ××•×¨×š ××—×™×“
   - ××˜×¨×™×¦×•×ª numpy ××•×›× ×•×ª ×œPyTorch

### 4. **Model Architecture Pipeline** ğŸ§ 
**×§×•×‘×¥**: `models/RNN_baseline.py`

#### ××¨×›×™×˜×§×˜×•×¨×ª ×”××•×“×œ:
```python
class LyricsRNN(nn.Module):
    def __init__(self):
        # 1. Embedding Layer (300D Word2Vec)
        self.embedding = nn.Embedding(vocab_size, 300)
        
        # 2. RNN Layer (LSTM/GRU)
        self.rnn = nn.LSTM(300, hidden_size, num_layers, dropout=0.3)
        
        # 3. Dropout Layer
        self.dropout_layer = nn.Dropout(0.3)
        
        # 4. Output Projection
        self.fc_out = nn.Linear(hidden_size, vocab_size)
```

#### ×–×¨×™××ª ×”××™×“×¢:
1. **Input**: ×¨×¦×£ ××™× ×“×§×¡×™× `[batch_size, seq_len]`
2. **Embedding**: `[batch_size, seq_len, 300]`
3. **RNN**: `[batch_size, seq_len, hidden_size]`
4. **Output**: `[batch_size, seq_len, vocab_size]`

### 5. **Training Pipeline** ğŸ‹ï¸â€â™‚ï¸
**×§×•×‘×¥**: `train.py` + `models/RNN_baseline.py`

#### LyricsRNNTrainer - ××—×œ×§×ª ×”××™××•×Ÿ:

**×©×œ×‘×™ ×”××™××•×Ÿ**:
1. **`train_step()`** - ×¦×¢×“ ××™××•×Ÿ ×™×—×™×“
   - Forward pass
   - ×—×™×©×•×‘ Loss (CrossEntropyLoss)
   - Backward propagation
   - Gradient clipping (max_norm=1.0)
   - Update weights

2. **`validate_step()`** - ×¦×¢×“ validation
   - Forward pass ×œ×œ× gradients
   - ×—×™×©×•×‘ validation loss

#### Training Loop ×‘×¤×•× ×§×¦×™×” `train_model()`:
```python
for epoch in range(num_epochs):
    # Training Phase
    for batch in train_loader:
        loss = trainer.train_step(input_batch, target_batch)
        
    # Validation Phase  
    for batch in val_loader:
        val_loss = trainer.validate_step(input_batch, target_batch)
        
    # Learning Rate Scheduling
    scheduler.step(avg_val_loss)
    
    # Early Stopping Check
    if val_loss < best_val_loss:
        save_best_model()
    else:
        patience_counter += 1
```

### 6. **Text Generation Pipeline** âœ¨
**×§×•×‘×¥**: `models/RNN_baseline.py`

#### ×¤×•× ×§×¦×™×™×ª `generate_text()`:
1. **Initialization**: ×˜×•×¢×Ÿ seed sequence
2. **Autoregressive Generation**:
   ```python
   for _ in range(max_length):
       # Forward pass
       output_logits = model(current_sequence)
       
       # Temperature scaling
       logits = logits / temperature
       
       # Top-k sampling
       top_k_logits, indices = torch.topk(logits, k)
       
       # Sample next word
       next_word = torch.multinomial(probabilities, 1)
       
       # Append to sequence
       generated_sequence.append(next_word)
   ```

### 7. **Evaluation Pipeline** ğŸ“Š
**×§×•×‘×¥**: `train.py`

#### ××“×“×™ ×”×¢×¨×›×”:
1. **Loss**: CrossEntropyLoss ×¢×œ test set
2. **Perplexity**: `exp(loss)` - ××“×“ ×œ××™ ×•×•×“××•×ª ×”××•×“×œ
3. **Generated Text Quality**: ×‘×“×™×§×” ××™×›×•×ª×™×ª ×©×œ ×˜×§×¡×˜ ×©× ×•×¦×¨

## ğŸ“ˆ ××˜×¨×™×§×•×ª ×•××“×“×™×

### Loss Function
```python
criterion = nn.CrossEntropyLoss(ignore_index=0)  # ××ª×¢×œ× ×-PAD tokens
```

### Perplexity Calculation
```python
perplexity = np.exp(cross_entropy_loss)
```
- ×¤×¨×¤×œ×§×¡×™×˜×™ × ××•×›×” = ××•×“×œ ×˜×•×‘ ×™×•×ª×¨
- ×¤×¨×¤×œ×§×¡×™×˜×™ ×©×œ ~100-200 × ×—×©×‘×ª ×˜×•×‘×” ×œ×’× ×¨×¦×™×™×ª ×˜×§×¡×˜

### Learning Rate Scheduling
```python
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)
```

## ğŸ›ï¸ ×”×™×¤×¨-×¤×¨××˜×¨×™×

```python
config = {
    'max_sequence_length': 50,    # ××•×¨×š ×¨×¦×£ ××§×¡×™××œ×™
    'batch_size': 32,             # ×’×•×“×œ batch
    'embedding_dim': 300,         # ×××“ Word2Vec
    'hidden_size': 512,           # ×’×•×“×œ hidden state
    'num_layers': 2,              # ××¡×¤×¨ ×©×›×‘×•×ª RNN
    'dropout': 0.3,               # ×§×¦×‘ dropout
    'learning_rate': 0.001,       # ×§×¦×‘ ×œ××™×“×”
    'min_word_freq': 2,           # ×ª×“×™×¨×•×ª ××™×œ×” ××™× ×™××œ×™×ª
}
```

## ğŸš€ ×”×¨×¦×ª ×”×¤×¨×•×™×§×˜

### ×“×¨×™×©×•×ª ××§×“×™××•×ª
```bash
pip install torch torchvision pandas numpy matplotlib seaborn gensim tqdm
```

### ×”×¨×¦×ª ××™××•×Ÿ
```bash
python train.py
```

### ×ª×”×œ×™×š ×”××™××•×Ÿ ×™×›×œ×•×œ:
1. âœ… ×˜×¢×™× ×ª × ×ª×•× ×™× ×•×¢×™×‘×•×“ ×˜×§×¡×˜
2. âœ… ×‘× ×™×™×ª ××™×œ×•×Ÿ ××™×œ×™×
3. âœ… ×˜×¢×™× ×ª Word2Vec embeddings  
4. âœ… ××™××•×Ÿ ×”××•×“×œ ×¢× early stopping
5. âœ… ×”×¢×¨×›×” ×¢×œ test set
6. âœ… ×’× ×¨×¦×™×™×ª ×“×•×’×××•×ª ×˜×§×¡×˜
7. âœ… ×©××™×¨×ª ××•×“×œ ×•××˜×¨×™×§×•×ª

## ğŸ“ ×¤×œ×˜×™× ×•×§×‘×¦×™×
- `models/best_lyrics_model.pth` - ×”××•×“×œ ×”×××•××Ÿ ×”×˜×•×‘ ×‘×™×•×ª×¨
- `models/preprocessor.pkl` - ×”preprocessor ×”×©××•×¨
- `training_curves.png` - ×’×¨×¤×™× ×©×œ loss ×•-perplexity
- Console output ×¢× ××“×“×™× ×•×“×•×’×××•×ª ×˜×§×¡×˜

## ğŸµ ×“×•×’×××•×ª ×’× ×¨×¦×™×”

×”××•×“×œ ×™×›×•×œ ×œ×™×¦×•×¨ ××™×œ×•×ª ×©×™×¨ ×‘×”×ª×‘×¡×¡ ×¢×œ ×˜×§×¡×˜ ×”×ª×—×œ×ª×™:

**Input**: "love is"  
**Output**: "love is a beautiful thing that makes me feel alive..."

**Input**: "in the night"  
**Output**: "in the night when stars are shining bright..."

## ğŸ”§ ×”×¨×—×‘×•×ª ×¢×ª×™×“×™×•×ª
- [ ] ××•×“×œ Transformer ×œ××™×œ×•×ª ×©×™×¨×™×
- [ ] ××™× ×˜×’×¨×¦×™×” ×¢× MIDI features
- [ ] ×××©×§ ××™× ×˜×¨××§×˜×™×‘×™ ×œ×’× ×¨×¦×™×”
- [ ] ××“×“×™ ×”×¢×¨×›×” ××™×›×•×ª×™×™× × ×•×¡×¤×™×

## ğŸ“š ××§×•×¨×•×ª ×•×”×©×¨××”
- ××¨×›×™×˜×§×˜×•×¨×ª RNN ××”×§×•×¨×¡ Deep Learning  
- Word2Vec embeddings ×-Google News
- ×˜×›× ×™×§×•×ª text generation ×¢×“×›× ×™×•×ª

---
**×¤×¨×•×™×§×˜ ×‘××¡×’×¨×ª**: ×”× ×“×¡×ª × ×ª×•× ×™× - ×œ××™×“×” ×¢××•×§×”, ×¡××¡×˜×¨ ×–'